{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "def pre_process():\n",
    "    data = pd.read_csv('WELFake_Dataset.csv', index_col=0)\n",
    "    print(data.shape)\n",
    "    # display(data[:300])\n",
    "    for i,x in data.iterrows():\n",
    "        if len(str(x[\"text\"])) <= 10:\n",
    "            data.loc[i, \"text\"] = np.nan\n",
    "        if len(str(x[\"title\"])) <= 10:\n",
    "            data.loc[i, \"title\"] = np.nan\n",
    "\n",
    "    data.dropna(inplace=True)\n",
    "    print(data.shape)\n",
    "    data.reset_index(drop=True, inplace=True)\n",
    "    data.to_csv(\"data/data.csv\")\n",
    "    display(data[:300])\n",
    "\n",
    "def tokenize():\n",
    "    stop = stopwords.words('english')\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    punc = [u'\\u201c',u'\\u201d',u'\\u2018',u'\\u2019',u'\\u2024',u'\\u2025',u'\\u2026',u'\\u2027']\n",
    "    print(punc)\n",
    "    data = pd.read_csv('data/data.csv', index_col=0)\n",
    "    data_cleaned = data.copy()\n",
    "    titles = list()\n",
    "    texts = list()\n",
    "    for i, row in data.iterrows():\n",
    "        title = str(row[\"title\"])\n",
    "        text = str(row[\"text\"])\n",
    "        t1 = \"\"\n",
    "        for c in title:\n",
    "            if not (c in string.punctuation or c in punc):\n",
    "                t1 += c\n",
    "            else:\n",
    "                t1 += \" \"\n",
    "        t2 = \"\"\n",
    "        for c in text:\n",
    "            if not (c in string.punctuation or c in punc):\n",
    "                t2 += c\n",
    "            else:\n",
    "                t2 += \" \"\n",
    "        title_tokens = nltk.tokenize.word_tokenize(t1)\n",
    "        text_tokens = nltk.tokenize.word_tokenize(t2)\n",
    "        # title_filtered = [w.lower() for w in title_tokens if not w.lower() in string.punctuation]\n",
    "        # title_filtered = [w.lower() for w in title_filtered if not w.lower() in punc]\n",
    "        title_filtered = [w.lower() for w in title_tokens if not w.lower() in stop]\n",
    "        title_stemmed = [stemmer.stem(w) for w in title_filtered]\n",
    "        # text_filtered = [w.lower() for w in text_tokens if not w.lower() in string.punctuation]\n",
    "        # text_filtered = [w.lower() for w in text_filtered if not w.lower() in punc]\n",
    "        text_filtered = [w.lower() for w in text_tokens if not w.lower() in stop]\n",
    "        text_stemmed = [stemmer.stem(w) for w in text_filtered]\n",
    "        # print(title_stemmed)\n",
    "        # print(text_stemmed)\n",
    "        titles.append(title_stemmed)\n",
    "        texts.append(text_stemmed)\n",
    "    data_cleaned[\"title\"] = titles\n",
    "    data_cleaned[\"text\"] = texts\n",
    "    data_cleaned.to_csv(\"data/data_token.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Bag of Words"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def make_bow(data_path):\n",
    "    data = pd.read_csv(data_path, index_col=0)\n",
    "    bow = []\n",
    "    bow_title = []\n",
    "    bow_text = []\n",
    "    bow_both = []\n",
    "    for i, row in data.iterrows():\n",
    "        words = row[\"title\"].split(\",\")\n",
    "        title = []\n",
    "        for word in words:\n",
    "            title.append(str(word).replace(\"'\", \"\").replace(\" \", \"\").replace(\"[\", \"\").replace(\"]\", \"\"))\n",
    "        words = row[\"text\"].split(\",\")\n",
    "        text = []\n",
    "        for word in words:\n",
    "            text.append(str(word).replace(\"'\", \"\").replace(\" \", \"\").replace(\"[\", \"\").replace(\"]\", \"\"))\n",
    "        dic_title = {}\n",
    "        dic_text = {}\n",
    "        dic_both = {}\n",
    "        for word in title:\n",
    "            if word in dic_title:\n",
    "                dic_title[word] = dic_title[word] + 1\n",
    "            else:\n",
    "                dic_title[word] = 1\n",
    "            if word in dic_both:\n",
    "                dic_both[word] = dic_both[word] + 1\n",
    "            else:\n",
    "                dic_both[word] = 1\n",
    "        for word in text:\n",
    "            if word in dic_text:\n",
    "                dic_text[word] = dic_text[word] + 1\n",
    "            else:\n",
    "                dic_text[word] = 1\n",
    "            if word in dic_both:\n",
    "                dic_both[word] = dic_both[word] + 1\n",
    "            else:\n",
    "                dic_both[word] = 1\n",
    "        bow_text.append(dic_text)\n",
    "        bow_title.append(dic_title)\n",
    "        bow_both.append(dic_both)\n",
    "    bow.append(bow_title)\n",
    "    bow.append(bow_text)\n",
    "    bow.append(bow_both)\n",
    "    return bow"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# bow = [bow_title[],bow_text[],bow_both[]]\n",
    "bow = make_bow('data_tokenized/data_token.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## TFIDF with Cosine"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "import math\n",
    "def tf(bow_):\n",
    "    tf_ = []\n",
    "    for dic in bow_:\n",
    "        max_ = 0\n",
    "        for i in dic:\n",
    "            if dic[i] > max_:\n",
    "                max_ = dic[i]\n",
    "        tf_dic = {}\n",
    "        for word in dic:\n",
    "            tf_dic[word] = dic[word]/max_\n",
    "        tf_.append(tf_dic)\n",
    "    return tf_\n",
    "\n",
    "def idf(bow_):\n",
    "    df_ = {}\n",
    "    for dic in bow_:\n",
    "        for word in dic:\n",
    "            if word in df_:\n",
    "                df_[word] += 1\n",
    "            else:\n",
    "                df_[word] = 1\n",
    "    idf_ = {}\n",
    "    for word in df_:\n",
    "        idf_[word] = math.log10(len(bow)/df_[word])\n",
    "    return idf_\n",
    "\n",
    "def tf_idf(bow_):\n",
    "    tf_ = tf(bow_)\n",
    "    idf_ = idf(bow_)\n",
    "    tfidf = []\n",
    "    for dic in tf_:\n",
    "        tfidf_dic = {}\n",
    "        for word in dic:\n",
    "            tfidf_dic[word] = dic[word] * idf_[word]\n",
    "        tfidf.append(tfidf_dic)\n",
    "    return tfidf\n",
    "\n",
    "def cosineSim(dic_a, dic_b):\n",
    "    for word in dic_a:\n",
    "        if word not in dic_b:\n",
    "            dic_b[word] = 0\n",
    "    for word in dic_b:\n",
    "        if word not in dic_a:\n",
    "            dic_a[word] = 0\n",
    "    dot, sum_a, sum_b = 0,0,0\n",
    "    for word in dic_a:\n",
    "        a = dic_a[word]\n",
    "        b = dic_b[word]\n",
    "        dot += (a*b)\n",
    "        sum_a += math.pow(a,2)\n",
    "        sum_b += math.pow(b,2)\n",
    "    sqrt_sum_a = math.sqrt(sum_a)\n",
    "    sqrt_sum_b = math.sqrt(sum_b)\n",
    "    return dot / (sqrt_sum_a * sqrt_sum_b)\n",
    "\n",
    "def tfidf_cosine_ranking(word_, bow_):\n",
    "    tfidf_all = tf_idf(bow_)\n",
    "    list_query = [{word_: 1}]\n",
    "    tfidf_query = tf_idf(list_query)[0]\n",
    "    article_index = []\n",
    "    cosSim = []\n",
    "    cos_index = 0\n",
    "    for a in tfidf_all:\n",
    "        article_index.append(cos_index)\n",
    "        cosSim.append(cosineSim(a,tfidf_query))\n",
    "        cos_index += 1\n",
    "    return pd.DataFrame({'article': article_index ,'value': cosSim }).sort_values(by=['value'], ascending=False)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "#cos_rank = tfidf_cosine_ranking('obama',bow[2])\n",
    "#print(cos_rank.head(5))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## bm25"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "from rank_bm25 import BM25Okapi\n",
    "def bm25_ranking(query_,index_):\n",
    "    data = pd.read_csv('data_tokenized/data_token.csv', index_col=0)\n",
    "    corpus = []\n",
    "    title = []\n",
    "    text = []\n",
    "    both = []\n",
    "    for i, row in data.iterrows():\n",
    "            words = row[\"title\"].split(\",\")\n",
    "            for word in words:\n",
    "                title.append(str(word).replace(\"'\", \"\").replace(\" \", \"\").replace(\"[\", \"\").replace(\"]\", \"\"))\n",
    "            words = row[\"text\"].split(\",\")\n",
    "            for word in words:\n",
    "                text.append(str(word).replace(\"'\", \"\").replace(\" \", \"\").replace(\"[\", \"\").replace(\"]\", \"\"))\n",
    "    if index_ == 0:\n",
    "        corpus = title\n",
    "    elif index_ == 1:\n",
    "        corpus = text\n",
    "    else:\n",
    "        for i in title:\n",
    "            both.append(title + text)\n",
    "        corpus = both\n",
    "\n",
    "    print(\"Starting bm25\")\n",
    "    bm25 = BM25Okapi(corpus)\n",
    "    bm25_scores = bm25.get_scores(query_.split(\" \"))\n",
    "\n",
    "    article_index = []\n",
    "    bm25_index = 0\n",
    "    for a in bm25_scores:\n",
    "        article_index.append(bm25_index)\n",
    "        bm25_index += 1\n",
    "    return pd.DataFrame({'article': article_index ,'value': bm25_scores }).sort_values(by=['value'], ascending=False)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "bm25_rank = bm25_ranking('sunday',bow[2])\n",
    "print(bm25_rank.head(5))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}