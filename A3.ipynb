{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from gensim.models import Word2Vec\n",
    "import multiprocessing\n",
    "from time import time\n",
    "from gensim import  models\n",
    "from gensim.test.utils import datapath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "def pre_process():\n",
    "    data = pd.read_csv('WELFake_Dataset.csv', index_col=0)\n",
    "    print(data.shape)\n",
    "    # display(data[:300])\n",
    "    for i,x in data.iterrows():\n",
    "        if len(str(x[\"text\"])) <= 10:\n",
    "            data.loc[i, \"text\"] = np.nan\n",
    "        if len(str(x[\"title\"])) <= 10:\n",
    "            data.loc[i, \"title\"] = np.nan\n",
    "\n",
    "    data.dropna(inplace=True)\n",
    "    print(data.shape)\n",
    "    data.reset_index(drop=True, inplace=True)\n",
    "    data.to_csv(\"data/data.csv\")\n",
    "    display(data[:300])\n",
    "\n",
    "def tokenize():\n",
    "    stop = stopwords.words('english')\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    punc = [u'\\u201c',u'\\u201d',u'\\u2018',u'\\u2019',u'\\u2024',u'\\u2025',u'\\u2026',u'\\u2027']\n",
    "    # print(punc)\n",
    "    data = pd.read_csv('data/data.csv', index_col=0)\n",
    "    data_cleaned = data.copy()\n",
    "    titles = list()\n",
    "    texts = list()\n",
    "    for i, row in data.iterrows():\n",
    "        title = str(row[\"title\"])\n",
    "        text = str(row[\"text\"])\n",
    "        t1 = \"\"\n",
    "        for c in title:\n",
    "            if not (c in string.punctuation or c in punc):\n",
    "                t1 += c\n",
    "            else:\n",
    "                t1 += \" \"\n",
    "        t2 = \"\"\n",
    "        for c in text:\n",
    "            if not (c in string.punctuation or c in punc):\n",
    "                t2 += c\n",
    "            else:\n",
    "                t2 += \" \"\n",
    "        title_tokens = nltk.tokenize.word_tokenize(t1)\n",
    "        text_tokens = nltk.tokenize.word_tokenize(t2)\n",
    "        # title_filtered = [w.lower() for w in title_tokens if not w.lower() in string.punctuation]\n",
    "        # title_filtered = [w.lower() for w in title_filtered if not w.lower() in punc]\n",
    "        title_filtered = [w.lower() for w in title_tokens if not w.lower() in stop]\n",
    "        title_stemmed = [stemmer.stem(w) for w in title_filtered]\n",
    "        # text_filtered = [w.lower() for w in text_tokens if not w.lower() in string.punctuation]\n",
    "        # text_filtered = [w.lower() for w in text_filtered if not w.lower() in punc]\n",
    "        text_filtered = [w.lower() for w in text_tokens if not w.lower() in stop]\n",
    "        text_stemmed = [stemmer.stem(w) for w in text_filtered]\n",
    "        # print(title_stemmed)\n",
    "        # print(text_stemmed)\n",
    "        titles.append(title_stemmed)\n",
    "        texts.append(text_stemmed)\n",
    "        if i % 5000 == 0:\n",
    "            print(i)\n",
    "    data_cleaned[\"title\"] = titles\n",
    "    data_cleaned[\"text\"] = texts\n",
    "    data_cleaned.to_csv(\"data/data_token.csv\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from disc.\n"
     ]
    }
   ],
   "source": [
    "load_model_from_disc = True\n",
    "\n",
    "try:\n",
    "    w2v_model = Word2Vec.load(\"word2vec.model\")\n",
    "except:\n",
    "    w2v_model = None\n",
    "\n",
    "if w2v_model is None or not load_model_from_disc:\n",
    "    if load_model_from_disc:\n",
    "        print(\"Could not load model from disc. Training model...\")\n",
    "    else:\n",
    "        print(\"Loading from disc deactivated. Training model...\")\n",
    "    data = pd.read_csv('data_tokenized/data_token.csv', index_col=0, dtype=str)\n",
    "\n",
    "    class MySentences(object):\n",
    "        def __init__(self, data):\n",
    "            self.data = data\n",
    "\n",
    "        def __iter__(self):\n",
    "            for doc in data[\"text\"]: #change to \"title\" or combine both\n",
    "                words = doc.split(\",\")\n",
    "                doc = []\n",
    "                for word in words:\n",
    "                    doc.append(str(word).replace(\"'\", \"\").replace(\" \", \"\").replace(\"[\", \"\").replace(\"]\", \"\"))\n",
    "                yield doc\n",
    "\n",
    "    sentences = MySentences(data)\n",
    "\n",
    "    cores = multiprocessing.cpu_count()\n",
    "    w2v_model = Word2Vec(min_count=20,\n",
    "                         window=2,\n",
    "                         sample=6e-5,\n",
    "                         alpha=0.03,\n",
    "                         min_alpha=0.0007,\n",
    "                         negative=20,\n",
    "                         workers=cores-1)\n",
    "\n",
    "    w2v_model.build_vocab(sentences, progress_per=10000)\n",
    "    t = time()\n",
    "    w2v_model.train(sentences, total_examples=w2v_model.corpus_count, epochs=3, report_delay=1)\n",
    "    print('Time to train the model: {} mins'.format(round((time() - t) / 60, 2)))\n",
    "    w2v_model.save(\"word2vec.model\")\n",
    "else:\n",
    "    print(\"Model loaded from disc.\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "data": {
      "text/plain": "0.026135037"
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate similarity\n",
    "w2v_model.wv.similarity(\"amazon\", 'nazi')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "data": {
      "text/plain": "0.61256015"
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate similarity\n",
    "w2v_model.wv.similarity(\"obama\", 'trump')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "data": {
      "text/plain": "'amazon'"
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find out which element doesn't match\n",
    "w2v_model.wv.doesnt_match(['amazon', 'obama', 'trump'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "data": {
      "text/plain": "[('barack', 0.6386590600013733),\n ('presid', 0.5025283098220825),\n ('predecessor', 0.45077192783355713)]"
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Which word is to obama as georg is to bush?\n",
    "w2v_model.wv.most_similar(positive=[\"obama\", \"georg\"], negative=[\"bush\"], topn=3)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "data": {
      "text/plain": "[('barack', 0.8123461604118347),\n ('predecessor', 0.6643136143684387),\n ('presid', 0.6526463627815247),\n ('administr', 0.6208781599998474),\n ('trump', 0.6125600934028625),\n ('undo', 0.5900983810424805),\n ('outgo', 0.5714931488037109),\n ('bush', 0.5544796586036682),\n ('rescind', 0.5267906785011292),\n ('holdov', 0.5253826379776001)]"
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# e.g. words most similar to obama\n",
    "w2v_model.wv.most_similar(positive=[\"obama\"])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "data": {
      "text/plain": "[('barack', 0.6946936845779419),\n ('obama', 0.6526462435722351),\n ('45th', 0.6525393724441528),\n ('successor', 0.6197038888931274),\n ('predecessor', 0.614696741104126),\n ('administr', 0.6129357218742371),\n ('trump', 0.6105669736862183),\n ('outgo', 0.6059844493865967),\n ('donald', 0.5954946279525757),\n ('presidenti', 0.5723046064376831)]"
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# e.g. words most similar to obama\n",
    "w2v_model.wv.most_similar(positive=[\"presid\"])"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
