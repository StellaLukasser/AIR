{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### AIR Project"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import ast\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from gensim.models import Word2Vec\n",
    "import multiprocessing\n",
    "from time import time"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "def pre_process():\n",
    "    data = pd.read_csv('WELFake_Dataset.csv', index_col=0)\n",
    "    print(data.shape)\n",
    "    # display(data[:300])\n",
    "    for i,x in data.iterrows():\n",
    "        if len(str(x[\"text\"])) <= 10:\n",
    "            data.loc[i, \"text\"] = np.nan\n",
    "        if len(str(x[\"title\"])) <= 10:\n",
    "            data.loc[i, \"title\"] = np.nan\n",
    "\n",
    "    data.dropna(inplace=True)\n",
    "    print(data.shape)\n",
    "    data.reset_index(drop=True, inplace=True)\n",
    "    data.to_csv(\"data/data.csv\")\n",
    "    display(data[:300])\n",
    "\n",
    "def tokenize():\n",
    "    stop = stopwords.words('english')\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    punc = [u'\\u201c',u'\\u201d',u'\\u2018',u'\\u2019',u'\\u2024',u'\\u2025',u'\\u2026',u'\\u2027']\n",
    "    # print(punc)\n",
    "    data = pd.read_csv('data/data.csv', index_col=0)\n",
    "    titles = list()\n",
    "    texts = list()\n",
    "    for i, row in data.iterrows():\n",
    "        title = str(row[\"title\"])\n",
    "        text = str(row[\"text\"])\n",
    "        t1 = \"\"\n",
    "        for c in title:\n",
    "            if not (c in string.punctuation or c in punc):\n",
    "                t1 += c\n",
    "            else:\n",
    "                t1 += \" \"\n",
    "        t2 = \"\"\n",
    "        for c in text:\n",
    "            if not (c in string.punctuation or c in punc):\n",
    "                t2 += c\n",
    "            else:\n",
    "                t2 += \" \"\n",
    "        title_tokens = nltk.tokenize.word_tokenize(t1)\n",
    "        text_tokens = nltk.tokenize.word_tokenize(t2)\n",
    "        # title_filtered = [w.lower() for w in title_tokens if not w.lower() in string.punctuation]\n",
    "        # title_filtered = [w.lower() for w in title_filtered if not w.lower() in punc]\n",
    "        title_filtered = [w.lower() for w in title_tokens if not w.lower() in stop]\n",
    "        title_stemmed = [stemmer.stem(w) for w in title_filtered]\n",
    "        # text_filtered = [w.lower() for w in text_tokens if not w.lower() in string.punctuation]\n",
    "        # text_filtered = [w.lower() for w in text_filtered if not w.lower() in punc]\n",
    "        text_filtered = [w.lower() for w in text_tokens if not w.lower() in stop]\n",
    "        text_stemmed = [stemmer.stem(w) for w in text_filtered]\n",
    "        # print(title_stemmed)\n",
    "        # print(text_stemmed)\n",
    "        titles.append(title_stemmed)\n",
    "        texts.append(text_stemmed)\n",
    "        if i % 5000 == 0:\n",
    "            print(i)\n",
    "    d = {\"title\":titles, \"text\":texts, \"label\":data[\"label\"]}\n",
    "    data_cleaned = pd.DataFrame(data=d)\n",
    "    # data_cleaned[\"title\"] = titles\n",
    "    # data_cleaned[\"text\"] = texts\n",
    "    data_cleaned.to_csv(\"data/data_token.csv\")\n",
    "# tokenize()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not load model from disc. Training model...\n",
      "Time to train the model: 1.82 mins\n"
     ]
    }
   ],
   "source": [
    "load_model_from_disc = True\n",
    "w2v_model = None\n",
    "if load_model_from_disc:\n",
    "    try:\n",
    "        w2v_model = Word2Vec.load(\"word2vec.model\")\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "if w2v_model is None or not load_model_from_disc:\n",
    "    if load_model_from_disc:\n",
    "        print(\"Could not load model from disc. Training model...\")\n",
    "    else:\n",
    "        print(\"Loading from disc deactivated. Training model...\")\n",
    "    data = pd.read_csv('data_tokenized/data_token.csv', index_col=0)#, dtype=str)\n",
    "\n",
    "    class MySentences(object):\n",
    "        def __init__(self, data):\n",
    "            self.data = data\n",
    "\n",
    "        def __iter__(self):\n",
    "            for doc in pd.concat([data[\"text\"], data[\"title\"]]): #change to \"title\" or combine both\n",
    "                doc = ast.literal_eval(doc)\n",
    "                yield doc\n",
    "\n",
    "    sentences = MySentences(data)\n",
    "\n",
    "    cores = multiprocessing.cpu_count()\n",
    "    w2v_model = Word2Vec(min_count=20,\n",
    "                         window=2,\n",
    "                         sample=6e-5,\n",
    "                         alpha=0.03,\n",
    "                         min_alpha=0.0007,\n",
    "                         negative=20,\n",
    "                         workers=cores-1)\n",
    "\n",
    "    w2v_model.build_vocab(sentences, progress_per=10000)\n",
    "    t = time()\n",
    "    w2v_model.train(sentences, total_examples=w2v_model.corpus_count, epochs=3, report_delay=1)\n",
    "    print('Time to train the model: {} mins'.format(round((time() - t) / 60, 2)))\n",
    "    w2v_model.save(\"word2vec.model\")\n",
    "else:\n",
    "    print(\"Model loaded from disc.\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "-0.11904364"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate similarity\n",
    "w2v_model.wv.similarity(\"amazon\", 'nazi')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "0.60530704"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate similarity\n",
    "w2v_model.wv.similarity(\"obama\", 'trump')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "'amazon'"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find out which element doesn't match\n",
    "w2v_model.wv.doesnt_match(['amazon', 'obama', 'trump'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "[('barack', 0.607993483543396),\n ('presid', 0.4728606641292572),\n ('behest', 0.4636874198913574)]"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Which word is to obama as georg is to bush?\n",
    "w2v_model.wv.most_similar(positive=[\"obama\", \"georg\"], negative=[\"bush\"], topn=3)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "[('barack', 0.832769513130188),\n ('administr', 0.6584988832473755),\n ('presid', 0.6397863626480103),\n ('predecessor', 0.625878632068634),\n ('trump', 0.6053071022033691),\n ('bush', 0.557529628276825),\n ('outgo', 0.5535241961479187),\n ('undo', 0.5471165180206299),\n ('holdov', 0.5300476551055908),\n ('clinton', 0.5224902629852295)]"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# e.g. words most similar to obama\n",
    "w2v_model.wv.most_similar(positive=[\"obama\"])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "[('barack', 0.7051833868026733),\n ('45th', 0.6659713387489319),\n ('successor', 0.6399291157722473),\n ('obama', 0.6397863626480103),\n ('administr', 0.6288126111030579),\n ('trump', 0.6117547750473022),\n ('donald', 0.6109979748725891),\n ('predecessor', 0.6096833348274231),\n ('pres', 0.594482958316803),\n ('presidenti', 0.5868942737579346)]"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# e.g. words most similar to obama\n",
    "w2v_model.wv.most_similar(positive=[\"presid\"])"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
