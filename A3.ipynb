{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### AIR Project"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Imports and specific settings"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "outputs": [],
   "source": [
    "import ast\n",
    "import string\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from gensim.models import Word2Vec\n",
    "import multiprocessing\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running models with cuda:0\n"
     ]
    }
   ],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "torch.backends.cudnn.benchmark = True\n",
    "print(\"running models with {}\".format(device))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Loading of dataset and nan-removal"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "outputs": [],
   "source": [
    "def pre_process():\n",
    "    data = pd.read_csv('WELFake_Dataset.csv', index_col=0)\n",
    "    print(data.shape)\n",
    "    # display(data[:300])\n",
    "    for i,x in data.iterrows():\n",
    "        if len(str(x[\"text\"])) <= 10:\n",
    "            data.loc[i, \"text\"] = np.nan\n",
    "        if len(str(x[\"title\"])) <= 10:\n",
    "            data.loc[i, \"title\"] = np.nan\n",
    "\n",
    "    data.dropna(inplace=True)\n",
    "    print(data.shape)\n",
    "    data.reset_index(drop=True, inplace=True)\n",
    "    data.to_csv(\"data/data.csv\")\n",
    "    display(data[:300])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Tokenization"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "outputs": [],
   "source": [
    "def tokenize():\n",
    "    stop = stopwords.words('english')\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    punc = [u'\\u201c',u'\\u201d',u'\\u2018',u'\\u2019',u'\\u2024',u'\\u2025',u'\\u2026',u'\\u2027']\n",
    "    # print(punc)\n",
    "    data = pd.read_csv('data/data.csv', index_col=0)\n",
    "    titles = list()\n",
    "    texts = list()\n",
    "    for i, row in data.iterrows():\n",
    "        title = str(row[\"title\"])\n",
    "        text = str(row[\"text\"])\n",
    "        t1 = \"\"\n",
    "        for c in title:\n",
    "            if not (c in string.punctuation or c in punc):\n",
    "                t1 += c\n",
    "            else:\n",
    "                t1 += \" \"\n",
    "        t2 = \"\"\n",
    "        for c in text:\n",
    "            if not (c in string.punctuation or c in punc):\n",
    "                t2 += c\n",
    "            else:\n",
    "                t2 += \" \"\n",
    "        title_tokens = nltk.tokenize.word_tokenize(t1)\n",
    "        text_tokens = nltk.tokenize.word_tokenize(t2)\n",
    "        # title_filtered = [w.lower() for w in title_tokens if not w.lower() in string.punctuation]\n",
    "        # title_filtered = [w.lower() for w in title_filtered if not w.lower() in punc]\n",
    "        title_filtered = [w.lower() for w in title_tokens if not w.lower() in stop]\n",
    "        title_stemmed = [stemmer.stem(w) for w in title_filtered]\n",
    "        # text_filtered = [w.lower() for w in text_tokens if not w.lower() in string.punctuation]\n",
    "        # text_filtered = [w.lower() for w in text_filtered if not w.lower() in punc]\n",
    "        text_filtered = [w.lower() for w in text_tokens if not w.lower() in stop]\n",
    "        text_stemmed = [stemmer.stem(w) for w in text_filtered]\n",
    "        # print(title_stemmed)\n",
    "        # print(text_stemmed)\n",
    "        titles.append(title_stemmed)\n",
    "        texts.append(text_stemmed)\n",
    "        if i % 5000 == 0:\n",
    "            print(i)\n",
    "    d = {\"title\":titles, \"text\":texts, \"label\":data[\"label\"]}\n",
    "    data_cleaned = pd.DataFrame(data=d)\n",
    "    # data_cleaned[\"title\"] = titles\n",
    "    # data_cleaned[\"text\"] = texts\n",
    "    data_cleaned.to_csv(\"data/data_token.csv\")\n",
    "# tokenize()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Creating and Training Word2Vec Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from disc.\n"
     ]
    }
   ],
   "source": [
    "load_model_from_disc = True\n",
    "w2v_model = None\n",
    "data = pd.read_csv('data_tokenized/data_token.csv', index_col=0)\n",
    "# for i, row in data.iterrows():\n",
    "#     print(type(row[\"title\"]))\n",
    "#     print(row)\n",
    "#     data.loc[i, \"title\"] = ast.literal_eval(row[\"title\"])\n",
    "#     data.loc[i, \"text\"] = ast.literal_eval(row[\"text\"])\n",
    "if load_model_from_disc:\n",
    "    try:\n",
    "        w2v_model = Word2Vec.load(\"word2vec.model\")\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "if w2v_model is None or not load_model_from_disc:\n",
    "    if load_model_from_disc:\n",
    "        print(\"Could not load model from disc. Training model...\")\n",
    "    else:\n",
    "        print(\"Loading from disc deactivated. Training model...\")\n",
    "\n",
    "    class MySentences(object):\n",
    "        def __init__(self, data):\n",
    "            self.data = data\n",
    "\n",
    "        def __iter__(self):\n",
    "            for doc in pd.concat([data[\"text\"], data[\"title\"]]): #change to \"title\" or combine both\n",
    "                doc = ast.literal_eval(doc)\n",
    "                yield doc\n",
    "\n",
    "    sentences = MySentences(data)\n",
    "\n",
    "    cores = multiprocessing.cpu_count()\n",
    "    w2v_model = Word2Vec(min_count=20,\n",
    "                         window=2,\n",
    "                         sample=6e-5,\n",
    "                         alpha=0.03,\n",
    "                         min_alpha=0.0007,\n",
    "                         negative=20,\n",
    "                         workers=cores-1)\n",
    "\n",
    "    w2v_model.build_vocab(sentences, progress_per=10000)\n",
    "    t = time.time()\n",
    "    w2v_model.train(sentences, total_examples=w2v_model.corpus_count, epochs=3, report_delay=1)\n",
    "    print('Time to train the model: {} mins'.format(round((time.time() - t) / 60, 2)))\n",
    "    w2v_model.save(\"word2vec.model\")\n",
    "else:\n",
    "    print(\"Model loaded from disc.\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "outputs": [
    {
     "data": {
      "text/plain": "-0.11904364"
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate similarity\n",
    "w2v_model.wv.similarity(\"amazon\", 'nazi')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "outputs": [
    {
     "data": {
      "text/plain": "0.60530704"
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate similarity\n",
    "w2v_model.wv.similarity(\"obama\", 'trump')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "outputs": [
    {
     "data": {
      "text/plain": "'amazon'"
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find out which element doesn't match\n",
    "w2v_model.wv.doesnt_match(['amazon', 'obama', 'trump'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "outputs": [
    {
     "data": {
      "text/plain": "[('barack', 0.607993483543396),\n ('presid', 0.4728606641292572),\n ('behest', 0.4636874198913574)]"
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Which word is to obama as georg is to bush?\n",
    "w2v_model.wv.most_similar(positive=[\"obama\", \"georg\"], negative=[\"bush\"], topn=3)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "outputs": [
    {
     "data": {
      "text/plain": "[('barack', 0.832769513130188),\n ('administr', 0.6584988832473755),\n ('presid', 0.6397863626480103),\n ('predecessor', 0.625878632068634),\n ('trump', 0.6053071022033691),\n ('bush', 0.557529628276825),\n ('outgo', 0.5535241961479187),\n ('undo', 0.5471165180206299),\n ('holdov', 0.5300476551055908),\n ('clinton', 0.5224902629852295)]"
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# e.g. words most similar to obama\n",
    "w2v_model.wv.most_similar(positive=[\"obama\"])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "outputs": [
    {
     "data": {
      "text/plain": "[('barack', 0.7051833868026733),\n ('45th', 0.6659713387489319),\n ('successor', 0.6399291157722473),\n ('obama', 0.6397863626480103),\n ('administr', 0.6288126111030579),\n ('trump', 0.6117547750473022),\n ('donald', 0.6109979748725891),\n ('predecessor', 0.6096833348274231),\n ('pres', 0.594482958316803),\n ('presidenti', 0.5868942737579346)]"
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# e.g. words most similar to obama\n",
    "w2v_model.wv.most_similar(positive=[\"presid\"])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Creating Doc2Vec\n",
    "word2vec for each word with average over document"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "outputs": [],
   "source": [
    "# creates w2v representation for all documents and titles\n",
    "def doc2vec():\n",
    "    titles = list()\n",
    "    texts = list()\n",
    "    start = time.time()\n",
    "    for i, row in data.iterrows():\n",
    "        vec_title = np.zeros(shape=w2v_model.vector_size)\n",
    "        vec_text = np.zeros(shape=w2v_model.vector_size)\n",
    "        tit = ast.literal_eval(row[\"title\"])\n",
    "        tex = ast.literal_eval(row[\"text\"])\n",
    "        tit_cnt = 0\n",
    "        tex_cnt = 0\n",
    "        for word in tit:\n",
    "            try:\n",
    "                vec_title += w2v_model.wv[word]\n",
    "            except KeyError:\n",
    "                # print(\"Didn't find word {}\".format(word))\n",
    "                tit_cnt += 1\n",
    "                pass\n",
    "        for word in tex:\n",
    "            try:\n",
    "                vec_text += w2v_model.wv[word]\n",
    "            except KeyError:\n",
    "                # print(\"Didn't find word {}\".format(word))\n",
    "                tex_cnt += 1\n",
    "                pass\n",
    "        if len(tit) > tit_cnt:\n",
    "            vec_title /= (len(tit) - tit_cnt)\n",
    "        if len(tex) > tex_cnt:\n",
    "            vec_text /= (len(tex) - tex_cnt)\n",
    "        titles.append(vec_title.tolist())\n",
    "        texts.append(vec_text.tolist())\n",
    "        if i % 5000 == 0:\n",
    "            print(\"[{}/{}] - {:.1f}s\".format(i, len(data.index), time.time() - start))\n",
    "    end = time.time()\n",
    "    print(\"creating doc2vec took {:.1f}s\".format(end - start))\n",
    "    d = {\"title\":titles, \"text\":texts, \"label\":data[\"label\"]}\n",
    "    data_w2v = pd.DataFrame(data=d)\n",
    "    data_w2v.to_csv(\"data/data_w2v.csv\")\n",
    "    display(data_w2v[:100])\n",
    "# doc2vec()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train-Test-split and Dataloader Creation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bf\n",
      "10\n",
      "tensor([[-4.1686e-01,  3.5883e-01,  3.4760e-02, -6.3746e-02,  2.4935e-02,\n",
      "         -9.5040e-02,  3.5321e-01,  1.0483e-01,  1.9505e-01,  2.8922e-01,\n",
      "         -3.6285e-02, -3.2419e-01, -7.1020e-01,  1.8390e-01,  8.2401e-03,\n",
      "         -6.8013e-01, -1.0447e-01,  1.6210e-01,  9.3961e-01, -7.0248e-01,\n",
      "          2.8800e-01,  4.5711e-01, -2.7507e-01, -4.0530e-02, -1.2603e-01,\n",
      "          5.4919e-01, -2.1448e-01, -3.0206e-01,  3.0307e-02,  1.1947e-02,\n",
      "          4.3648e-01, -2.5218e-02, -2.8998e-01, -5.0695e-01,  9.8252e-02,\n",
      "          2.4983e-01, -1.5599e-01, -9.4060e-01, -2.9541e-01, -1.6763e-01,\n",
      "         -1.7932e-01,  1.3420e-01, -1.8323e-02,  1.3620e-01,  4.6321e-01,\n",
      "         -4.4259e-01, -3.7899e-01, -5.4660e-01,  9.8054e-01,  1.1294e-01,\n",
      "         -1.4967e-01, -3.6480e-03,  8.3331e-01, -3.5666e-02,  1.8833e-01,\n",
      "         -1.1445e-01,  1.5629e-01,  2.3504e-01, -3.4132e-01, -1.0942e-02,\n",
      "         -1.4456e-01, -4.8057e-02,  4.3837e-01, -9.8660e-03, -1.3837e-01,\n",
      "          3.2350e-01, -2.3701e-01, -5.1720e-01, -1.8908e-01,  9.4693e-01,\n",
      "          9.8712e-02,  2.2407e-01,  2.9059e-01,  3.3490e-01,  1.8234e-02,\n",
      "          4.6592e-01, -1.0276e-01, -1.6157e-01,  5.4565e-02, -2.6838e-01,\n",
      "         -2.9728e-01,  2.6321e-01, -8.3809e-01,  7.9244e-02, -1.1027e-01,\n",
      "          2.8613e-02,  3.1812e-01, -5.8427e-01,  8.1861e-01, -3.1064e-01,\n",
      "          6.5042e-02, -2.4459e-01,  1.2923e-01, -2.6682e-01, -3.1882e-01,\n",
      "          5.1305e-01,  1.6687e-01, -1.0390e+00, -2.7736e-01,  2.3336e-02],\n",
      "        [-1.2269e-01,  2.0471e-01, -6.5489e-01, -1.0483e+00, -1.3434e-02,\n",
      "          5.1398e-01,  5.1974e-01,  8.4759e-02,  4.6516e-02,  5.1957e-01,\n",
      "         -4.4632e-01, -7.0560e-01, -9.7757e-02,  1.9029e-01, -2.2631e-01,\n",
      "          5.4452e-02, -1.2018e-01, -2.8346e-02,  5.8492e-01, -8.7987e-02,\n",
      "          3.4734e-01,  9.6687e-01,  2.2545e-01,  1.1090e-01,  2.2562e-01,\n",
      "          2.8222e-01, -3.0336e-01, -8.5447e-01, -3.3444e-01, -1.5254e-01,\n",
      "          2.2569e-01,  1.4207e-01, -5.9412e-02, -7.5479e-01,  1.3943e-01,\n",
      "          2.4676e-01, -5.0622e-02, -2.4280e-01, -6.1683e-01, -2.7200e-01,\n",
      "         -2.2281e-01,  2.9183e-01, -2.5817e-01,  1.7209e-02,  9.3144e-02,\n",
      "         -3.7065e-01, -1.1317e-01, -5.0270e-01, -1.3254e-01,  5.1416e-01,\n",
      "          1.9878e-01, -6.5163e-01,  3.9244e-01,  6.9058e-01, -1.4779e-01,\n",
      "         -2.3727e-01, -4.3505e-01, -1.5416e-01, -6.0753e-01, -3.0725e-01,\n",
      "          2.0167e-01,  2.3152e-01, -2.4065e-01,  2.6470e-01, -3.2848e-01,\n",
      "         -2.0535e-01, -1.8665e-01, -7.0772e-01, -4.1529e-02,  7.8023e-01,\n",
      "          2.6914e-01,  1.4258e-01, -2.8149e-01,  4.4063e-01,  3.0932e-01,\n",
      "          3.7109e-01, -2.2838e-01, -3.0169e-01, -5.9606e-01, -3.0405e-01,\n",
      "         -2.2276e-01,  1.5784e-01, -4.1757e-01,  6.5313e-01, -6.1044e-01,\n",
      "         -9.8458e-02,  4.5014e-01, -3.2531e-01, -1.0817e-02, -1.5014e-01,\n",
      "         -1.5387e-01,  4.7302e-01,  4.8087e-01, -2.4561e-01, -1.0162e+00,\n",
      "          2.8173e-01,  1.5446e-01, -8.4027e-01, -1.1027e-01, -6.2774e-01],\n",
      "        [-7.9835e-02,  2.8497e-01, -6.8464e-01,  1.7832e-01,  5.7786e-01,\n",
      "          1.7494e-01,  8.6569e-01,  3.6764e-01,  3.8340e-01, -7.1693e-02,\n",
      "          2.3802e-01, -2.9228e-01, -3.5844e-01,  6.9623e-01, -1.7833e-01,\n",
      "         -4.8977e-01, -4.9241e-01,  1.9128e-01,  4.8371e-02, -1.1828e-01,\n",
      "          4.7938e-01,  1.0423e+00,  3.1226e-01,  2.8936e-01,  2.3659e-01,\n",
      "          3.0984e-01, -5.8604e-01, -4.9424e-01,  5.3914e-01,  5.8571e-01,\n",
      "         -1.5477e-01,  8.5744e-01,  1.0757e-01, -4.8692e-01, -1.1173e-01,\n",
      "          8.7176e-02,  2.6890e-01, -8.1355e-01, -9.3406e-02, -1.0673e+00,\n",
      "         -1.2364e-01, -4.3045e-01, -7.0960e-01, -1.9975e-01, -6.4023e-02,\n",
      "         -5.2771e-01,  2.5699e-01, -4.7976e-01, -3.0925e-01,  2.1290e-01,\n",
      "          5.3055e-03, -2.4181e-01, -2.2406e-01,  9.6998e-02,  1.0277e-01,\n",
      "          7.3851e-01, -3.2234e-02, -5.0484e-01, -8.9540e-01,  4.2078e-02,\n",
      "          3.0338e-01,  6.1634e-01, -5.1140e-02,  6.6386e-01,  5.1989e-01,\n",
      "         -3.5068e-03, -2.0298e-01,  3.7002e-02, -2.6899e-01,  7.3193e-01,\n",
      "         -1.4640e-01,  6.1606e-01, -2.7523e-01,  3.7291e-01, -2.5154e-01,\n",
      "         -2.7016e-01, -6.1909e-01, -5.3343e-01, -1.4104e+00,  1.0923e-01,\n",
      "         -1.4340e+00, -1.8794e-01,  2.7212e-02,  6.2267e-01, -4.8430e-01,\n",
      "         -2.6159e-01, -4.3258e-01, -5.2893e-01,  1.0910e-01,  8.0802e-01,\n",
      "          8.5267e-02,  7.5050e-02, -6.1364e-02,  1.8287e-01, -2.5939e-01,\n",
      "         -1.6854e-01,  8.8174e-01, -4.2412e-01, -1.1692e-01, -1.9522e-01],\n",
      "        [-5.4403e-01,  4.0059e-01, -4.1087e-02, -2.9957e-01,  1.8482e-01,\n",
      "         -2.1953e-01,  2.7891e-01, -1.0296e-02,  2.1714e-01,  4.3089e-01,\n",
      "         -4.3993e-01, -8.3506e-01, -1.6312e-01,  3.5740e-02, -4.2872e-01,\n",
      "         -5.7400e-01,  2.3332e-01, -1.5440e-02,  5.9852e-02, -2.5867e-01,\n",
      "          1.1818e-01,  7.3695e-01, -2.8061e-01,  3.8881e-01, -3.5828e-02,\n",
      "          2.4510e-01, -4.0660e-01, -7.3145e-01, -6.5824e-01, -8.0857e-02,\n",
      "          1.7048e-01,  4.3274e-01, -1.9065e-01, -2.9767e-01,  1.2533e-01,\n",
      "          1.7979e-01, -3.7554e-02, -2.3174e-01, -3.6833e-01, -1.6103e-01,\n",
      "          2.3400e-01, -4.6714e-01,  2.5495e-01,  4.2599e-02,  4.6128e-01,\n",
      "         -5.4519e-01, -2.8529e-02, -3.7216e-01,  6.7574e-01, -7.4928e-02,\n",
      "          4.2334e-01, -3.5941e-01,  4.0037e-01, -8.0647e-02,  2.5032e-01,\n",
      "         -7.2150e-02, -1.7926e-01,  4.2811e-03, -4.7465e-01, -5.2788e-01,\n",
      "         -1.8827e-01, -5.4811e-01,  1.7464e-01,  1.7486e-01, -2.2717e-01,\n",
      "         -2.6819e-01, -8.8003e-01, -1.7818e-01, -3.8491e-01,  5.9393e-01,\n",
      "          2.7190e-01,  2.7702e-01, -3.0245e-01,  3.7516e-01, -4.9775e-01,\n",
      "          5.9377e-01, -6.8101e-02,  8.3855e-02, -9.8183e-02, -6.4720e-01,\n",
      "          1.1632e-01, -3.0241e-01, -4.7496e-01,  7.5989e-01, -1.0586e-01,\n",
      "          3.2282e-01,  2.6740e-01,  1.3855e-01, -3.7544e-02,  1.9562e-02,\n",
      "         -1.2090e-01,  8.3829e-03,  2.5084e-01,  4.3782e-02, -5.6166e-01,\n",
      "         -1.0079e-01,  3.3607e-01, -6.1485e-01,  3.3163e-01, -6.7081e-01],\n",
      "        [-4.2236e-01,  5.3980e-01, -4.3796e-01, -3.3150e-01, -1.4018e-01,\n",
      "         -2.1353e-01,  2.7982e-01, -2.0635e-01, -1.7310e-01,  2.3886e-01,\n",
      "         -1.2790e-01, -4.9360e-01, -4.2034e-01,  6.0562e-02, -2.9368e-01,\n",
      "         -1.8547e-01,  2.9058e-01, -1.6117e-01,  5.1150e-01, -7.1118e-01,\n",
      "         -6.9502e-02,  5.2900e-01, -3.0957e-01,  8.4773e-01, -4.9963e-01,\n",
      "          5.9611e-02, -1.5160e-01, -5.8850e-02,  5.1660e-01,  6.3908e-01,\n",
      "          5.2449e-01,  5.7224e-01,  1.0323e-01, -7.0143e-01, -4.2433e-02,\n",
      "          3.4365e-01, -3.5252e-01, -6.5478e-01,  8.5208e-03, -5.2026e-01,\n",
      "         -3.5428e-01, -2.1831e-01, -1.9608e-01, -6.3712e-01, -4.1385e-01,\n",
      "         -5.6923e-01,  2.8983e-01, -1.1844e-01,  4.2043e-02,  4.9477e-01,\n",
      "          5.1005e-01, -4.1022e-01,  8.9185e-01,  1.1590e-01, -5.7072e-01,\n",
      "         -4.0794e-01, -1.2296e-01, -1.6620e-02, -4.6137e-01,  3.2636e-01,\n",
      "         -3.6057e-01,  2.6697e-01, -1.2742e-01,  7.3250e-01, -2.3288e-01,\n",
      "         -3.2799e-01, -5.1040e-01,  1.7987e-01,  4.9092e-01,  4.9488e-01,\n",
      "          2.7572e-01,  4.9001e-02, -3.2615e-01,  2.3886e-01, -4.5503e-01,\n",
      "          7.4641e-01, -5.8564e-01,  2.9069e-01, -4.3164e-02, -1.1791e-01,\n",
      "          1.2924e-01, -8.6840e-02, -2.8191e-01,  7.9257e-01, -1.5181e-01,\n",
      "         -2.3293e-02,  5.3663e-01, -2.5682e-01,  3.4490e-01, -1.0736e-01,\n",
      "         -1.4889e-01,  1.4451e-01,  4.4549e-01, -7.8300e-02, -1.0991e-01,\n",
      "          1.5492e-01,  4.5104e-01, -9.9501e-01, -1.6649e-01, -3.6795e-01],\n",
      "        [-6.0197e-01,  1.5843e-02,  9.7218e-02, -1.2639e-01,  5.1790e-01,\n",
      "         -2.3440e-01,  2.8888e-02,  2.0000e-01, -2.7949e-01, -4.6224e-01,\n",
      "          1.9324e-01, -8.3280e-01, -1.3611e-01,  5.3657e-01, -2.7568e-01,\n",
      "         -1.7636e-02,  1.6204e-01, -8.0229e-02, -3.0537e-01, -2.1711e-01,\n",
      "          6.0577e-02,  1.3243e-01,  3.6047e-01,  2.2215e-01, -1.0373e-01,\n",
      "          6.2671e-03, -3.2212e-01, -3.7391e-01, -2.2529e-01, -3.5525e-02,\n",
      "          3.7370e-01,  7.6920e-02, -2.1900e-01, -2.7550e-01, -9.5388e-02,\n",
      "         -1.2305e-01, -3.4554e-01, -4.7043e-01, -1.0303e-01, -6.5294e-02,\n",
      "          1.6783e-01, -1.5182e-01,  2.4515e-01,  1.1046e-01,  2.4836e-01,\n",
      "         -5.4502e-01, -1.2026e-01, -7.6388e-01,  2.1760e-01,  1.5089e-01,\n",
      "          8.3734e-02, -1.2958e-01,  4.6982e-01,  3.7702e-01, -4.2859e-02,\n",
      "         -2.8353e-01, -7.8334e-02,  6.9348e-02, -1.7343e-01, -1.5752e-01,\n",
      "          5.8297e-01,  2.6937e-01,  3.3578e-01,  2.7459e-01,  1.3807e-01,\n",
      "          1.0075e-01, -1.3331e-02,  2.3381e-01, -3.8294e-01,  3.2032e-01,\n",
      "         -6.4382e-02,  1.6785e-01, -7.4404e-02, -9.0523e-02,  4.6699e-01,\n",
      "          4.6367e-01,  9.8290e-02,  9.3578e-02,  1.9549e-01,  1.2943e-01,\n",
      "         -2.7929e-01,  1.1209e-01, -6.5808e-01,  4.6655e-01, -4.3297e-01,\n",
      "         -1.5756e-01,  5.3151e-02, -5.9004e-01, -1.3719e-01,  2.1881e-01,\n",
      "         -4.1588e-01,  3.9724e-01,  5.8477e-02, -1.7950e-01,  4.8514e-01,\n",
      "          1.8148e-01,  2.2884e-01, -1.1541e-02, -2.8786e-02, -6.8288e-01],\n",
      "        [-2.0980e-01,  7.1139e-01,  2.3980e-01,  3.3171e-01, -3.3343e-01,\n",
      "          5.1158e-01,  6.0264e-02,  2.5811e-01,  7.0079e-01,  2.2932e-02,\n",
      "         -9.0134e-01, -6.2656e-01, -6.4039e-01,  3.9998e-01,  2.1269e-01,\n",
      "         -6.3659e-01,  2.5189e-01,  3.8820e-02,  3.9115e-01, -4.2857e-01,\n",
      "         -1.2274e-01,  4.8488e-01, -2.0559e-01,  5.3954e-01, -7.2532e-01,\n",
      "          5.2806e-01, -6.3803e-01, -9.1129e-02, -5.7175e-01, -1.8592e-01,\n",
      "          2.5894e-01, -1.6131e-01, -8.4896e-01, -1.8764e-01, -7.4070e-01,\n",
      "          2.4152e-01,  1.0401e-01,  1.6430e-01, -2.5531e-01, -3.5993e-01,\n",
      "         -4.4209e-01, -2.3004e-01, -4.9687e-01,  6.6175e-01,  2.4534e-01,\n",
      "          4.8452e-03, -7.0757e-02, -8.0597e-01,  1.2106e+00,  5.7393e-01,\n",
      "         -6.3422e-01,  5.3804e-01,  5.7307e-01, -2.6618e-01, -3.6577e-01,\n",
      "         -2.0443e-01, -2.7313e-01,  1.6564e-01, -5.1144e-01, -1.9593e-01,\n",
      "          5.2658e-02,  5.2897e-02, -3.1486e-03,  5.0538e-03, -4.6196e-01,\n",
      "          8.3823e-02, -3.3677e-01, -6.1994e-01, -4.8369e-02,  5.7157e-01,\n",
      "         -5.1297e-02,  5.6788e-01,  4.6815e-01,  5.3621e-01, -1.8642e-01,\n",
      "          4.1364e-01, -1.8847e-01,  4.4264e-02,  1.1925e+00,  7.3209e-02,\n",
      "         -6.5371e-01,  1.0732e-02, -1.5592e-01,  5.0752e-01,  6.0782e-01,\n",
      "          1.4822e-01,  2.9666e-02,  4.0679e-01,  1.8528e-01, -8.4955e-01,\n",
      "          6.5306e-01, -3.9937e-01,  2.6321e-01,  3.0416e-02,  3.3423e-01,\n",
      "          2.3048e-01,  8.8641e-02, -3.4363e-01,  2.0458e-01, -1.6292e-01],\n",
      "        [-1.2737e+00,  4.2005e-01,  2.3500e-01,  1.0570e-02,  2.6688e-01,\n",
      "          3.3125e-02,  4.0819e-01, -1.0224e-01,  3.7743e-01,  4.6472e-01,\n",
      "         -2.2101e-01, -1.7362e-01, -3.8779e-01, -1.5647e-02, -1.5781e-01,\n",
      "         -3.1582e-01, -3.1762e-01,  3.2335e-01,  4.5323e-01, -5.0227e-01,\n",
      "          4.3285e-01, -1.7444e-01, -2.2148e-01, -2.5972e-01, -5.2556e-01,\n",
      "          2.0596e-01, -1.7540e-01, -2.7647e-01, -3.3541e-01, -2.6163e-01,\n",
      "          5.8455e-02,  6.2067e-02, -3.1725e-01, -2.9012e-01,  2.3182e-01,\n",
      "          3.7653e-01, -1.9468e-01, -5.9520e-01, -4.7375e-02,  4.1760e-02,\n",
      "          8.6546e-02, -2.0424e-01,  1.4704e-01,  2.5454e-01,  2.3669e-01,\n",
      "         -7.7958e-01, -9.9146e-02, -4.2811e-01,  4.8242e-01,  1.2900e-02,\n",
      "         -3.3986e-01,  5.7389e-02,  5.3648e-01,  3.9547e-01, -3.5697e-01,\n",
      "         -2.4261e-01,  1.5776e-01, -2.1856e-02, -3.4443e-03, -1.6529e-01,\n",
      "          7.5764e-02, -2.2314e-01,  5.0038e-01, -1.9994e-02, -8.2752e-02,\n",
      "         -6.7742e-02,  1.5765e-01,  2.0042e-01,  8.5806e-02,  2.8133e-02,\n",
      "         -5.9989e-01, -2.1587e-02,  4.5460e-01,  8.8607e-02, -2.6257e-01,\n",
      "          2.1828e-01, -9.9769e-06, -5.0122e-01, -4.2122e-01, -7.0895e-01,\n",
      "         -2.9877e-01, -2.5339e-01, -8.4827e-02,  3.1515e-01,  1.4672e-01,\n",
      "          4.1226e-02,  2.4548e-01,  6.7979e-02,  2.7131e-01,  5.0636e-02,\n",
      "         -4.2804e-01, -3.1558e-01,  2.1429e-01, -1.5882e-01,  7.4113e-02,\n",
      "          6.1717e-01,  5.2586e-01, -4.0622e-01, -1.8880e-01, -9.4546e-02],\n",
      "        [-3.2798e-01,  5.3973e-01, -1.4496e-01, -4.9892e-01, -5.1251e-02,\n",
      "          4.4095e-01,  7.7342e-01,  1.8804e-01,  3.9471e-01,  7.0794e-02,\n",
      "         -2.9998e-02, -6.0772e-01,  3.7817e-01,  7.7316e-03, -9.9974e-03,\n",
      "         -1.4023e-01,  1.3549e-02, -1.7342e-01,  4.8010e-01,  9.2804e-02,\n",
      "          6.9568e-01,  5.6728e-01,  2.3097e-01, -2.0161e-02,  1.0809e-01,\n",
      "          2.4526e-01, -3.7382e-01, -3.7989e-01, -4.1511e-01,  8.4111e-02,\n",
      "         -2.8633e-01,  4.3114e-01, -4.1099e-01, -1.0071e+00,  4.9623e-02,\n",
      "         -5.5679e-01, -7.6149e-01, -2.1028e-01,  7.3217e-02,  3.0494e-01,\n",
      "         -2.1423e-01, -2.7664e-01,  8.6099e-02,  2.6914e-01,  4.3925e-01,\n",
      "         -3.2755e-01, -6.5045e-01, -3.7556e-01, -2.8625e-01, -9.0053e-03,\n",
      "         -4.6187e-02, -5.7418e-01,  5.7798e-01,  1.5934e-01,  1.5236e-02,\n",
      "         -3.4966e-01, -3.2323e-01,  8.0807e-02, -4.6492e-01,  1.2020e-02,\n",
      "          1.3795e-01,  1.5449e-01, -3.1526e-01,  2.4171e-01, -1.3880e-01,\n",
      "          5.0078e-02, -5.6848e-01,  3.6404e-01, -4.6116e-01,  6.8022e-02,\n",
      "         -3.7517e-01,  8.0461e-01,  2.5629e-01,  4.9398e-01, -6.6830e-02,\n",
      "          9.0045e-02, -2.2959e-02, -3.5321e-01,  9.5210e-02, -4.3405e-01,\n",
      "         -1.4477e-01, -6.2059e-01, -7.3305e-01,  9.0295e-02, -2.6886e-01,\n",
      "          4.4116e-01,  1.7587e-01, -1.2030e-01, -2.3510e-01, -1.6863e-01,\n",
      "          3.4861e-01,  1.2746e-01,  1.4896e-01,  1.6578e-01, -2.1990e-01,\n",
      "          1.2191e-02,  3.1902e-01, -9.7703e-01, -1.4275e-01, -8.7980e-01],\n",
      "        [ 6.6031e-02,  3.2170e-01,  3.0052e-01, -1.3462e-01, -2.1674e-01,\n",
      "          1.9757e-01,  2.0725e-01, -4.2967e-01,  2.4857e-02, -1.2104e-01,\n",
      "         -6.1011e-01, -1.9563e-01, -5.2887e-02,  3.6834e-01, -1.3340e-01,\n",
      "         -6.8200e-01, -2.3821e-01, -8.7416e-01,  1.0986e-01, -7.0270e-01,\n",
      "          7.7302e-01,  6.6892e-02, -5.2259e-02,  2.7519e-01, -2.9308e-01,\n",
      "          1.8648e-01, -2.3689e-01,  1.7400e-01, -2.2098e-01,  2.6579e-01,\n",
      "          4.2945e-01,  5.5936e-02,  6.4351e-02, -8.6733e-01,  3.4144e-01,\n",
      "          3.5070e-01, -5.2947e-01,  8.4574e-02,  2.4346e-01, -2.6695e-01,\n",
      "         -7.6369e-02,  3.0621e-01,  3.0732e-01,  3.5633e-01,  4.5661e-01,\n",
      "         -2.3159e-01, -1.6125e-01, -7.1184e-01,  5.5907e-01,  5.1029e-01,\n",
      "          5.6613e-01, -1.7724e-01,  1.5088e-01, -2.6535e-01, -2.6346e-01,\n",
      "          5.1470e-01,  2.5646e-01,  1.5658e-01,  3.2752e-01, -3.4620e-01,\n",
      "         -2.9218e-01,  3.5469e-01, -4.7957e-01,  7.8573e-02,  4.0678e-03,\n",
      "         -2.9840e-01, -9.0929e-02,  6.2615e-01, -1.0660e-01,  5.4781e-01,\n",
      "         -2.8218e-01,  5.4191e-01,  6.3198e-01,  9.4937e-02,  2.3369e-01,\n",
      "          6.0585e-01,  4.5767e-02,  3.0999e-01,  4.3776e-01,  7.6142e-01,\n",
      "         -2.9329e-01, -7.7151e-01, -2.6760e-01,  1.1486e-01,  5.4540e-01,\n",
      "         -1.3170e-01,  7.6348e-01,  9.9139e-02,  8.0267e-02, -7.9827e-01,\n",
      "          3.9864e-01,  2.9570e-01,  6.2688e-01, -3.9182e-01,  2.2458e-01,\n",
      "         -3.0577e-01,  1.5077e-01,  4.2104e-02,  9.0616e-02,  7.2259e-02]])\n",
      "torch.Size([10, 100])\n",
      "torch.Size([10])\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "params = {'batch_size': 10,\n",
    "          'shuffle': True,\n",
    "          'num_workers': 0}\n",
    "max_epochs = 100\n",
    "\n",
    "\n",
    "data_d2v = pd.read_csv(\"data/data_w2v.csv\", index_col=0)\n",
    "titles = list()\n",
    "texts = list()\n",
    "# print(\"interpreting data\")\n",
    "# for i, row in data_d2v.iterrows():\n",
    "#     titles.append(ast.literal_eval(row[\"title\"]))\n",
    "#     texts.append(ast.literal_eval(row[\"text\"]))\n",
    "# print(\"done interpreting data\")\n",
    "# data_d2v[\"title\"] = titles\n",
    "# data_d2v[\"text\"] = texts\n",
    "\n",
    "\n",
    "data_d2v_title = data_d2v[[\"title\", \"label\"]].copy()\n",
    "data_d2v_text = data_d2v[[\"text\", \"label\"]].copy()\n",
    "X_train_title, X_test_title, y_train_title, y_test_title = train_test_split(data_d2v_title[\"title\"], data_d2v_title[\"label\"], test_size=0.15, random_state=42, shuffle=True)\n",
    "X_train_text, X_test_text, y_train_text, y_test_text = train_test_split(data_d2v_text[\"text\"], data_d2v_text[\"label\"], test_size=0.15, random_state=42, shuffle=True)\n",
    "\n",
    "X_train_title.reset_index(drop=True, inplace=True)\n",
    "X_test_title.reset_index(drop=True, inplace=True)\n",
    "y_train_title.reset_index(drop=True, inplace=True)\n",
    "y_test_title.reset_index(drop=True, inplace=True)\n",
    "X_train_text.reset_index(drop=True, inplace=True)\n",
    "X_test_text.reset_index(drop=True, inplace=True)\n",
    "y_train_text.reset_index(drop=True, inplace=True)\n",
    "y_test_text.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "class data_set(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        super(Dataset, self).__init__()\n",
    "        assert len(X.index) == len(y.index)\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X.index)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return ast.literal_eval(self.X[index]), self.y[index]\n",
    "\n",
    "def collate_batch(batch):\n",
    "    labels = list()\n",
    "    texts = list()\n",
    "    for (_text, _label) in batch:\n",
    "        labels.append(_label)\n",
    "        texts.append(_text)\n",
    "\n",
    "    return torch.tensor(texts), torch.tensor(labels)\n",
    "\n",
    "    # print(batch)\n",
    "\n",
    "train_dataset_title = data_set(X_train_title, y_train_title)\n",
    "test_dataset_title = data_set(X_test_title, y_test_title)\n",
    "train_dataset_text = data_set(X_train_text, y_train_text)\n",
    "test_dataset_text = data_set(X_test_text, y_test_text)\n",
    "\n",
    "train_dataloader_title = DataLoader(train_dataset_title, **params, collate_fn=collate_batch)\n",
    "test_dataloader_title = DataLoader(test_dataset_title, **params)\n",
    "train_dataloader_text = DataLoader(train_dataset_text, **params)\n",
    "test_dataloader_text = DataLoader(test_dataset_text, **params)\n",
    "\n",
    "# for batch, (X, y) in enumerate(train_dataloader_title):\n",
    "#     print(X)\n",
    "#     print(X.shape)\n",
    "#     print(y.shape)\n",
    "#     break\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
